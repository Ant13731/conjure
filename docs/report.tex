\documentclass{article}

\usepackage{minted}
\usepackage{amsmath, amsthm}
\usepackage{amssymb}
\usepackage{listings}
\usepackage[svgnames]{xcolor}
\usepackage{tikz}
\usepackage{array}
\usepackage{graphicx}
\usepackage[backend=biber, maxbibnames=9]{biblatex}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{mathtools}
\usepackage[rightcaption]{sidecap}
\usepackage{subcaption}
\usepackage{hyperref}

\graphicspath{ {./images/} }

\addbibresource{report.bib}

\title{Conjure: A Computer Vision Controller Using Hand Gestures}
\author{Anthony Hunt}

\begin{document}
\maketitle

\section{Introduction}
Over the last 20 years, extraordinary progress in image processing and CNNs have brought about revolutionary methods of interacting with the world. From self-driving cars to early cancer detection, computer vision has become integral to interactions with technology and our environments. The usefulness of such software is self-evident in its pervasiveness within smartphones and personal computing; home buttons have been replaced with retina scanners and facial recognition, VR headsets no longer need dedicated controllers, and any device can read handwritten text directly from a photo.

In the gradual rollout of virtual hands-free computing, gesture-based interactions serve as a natural and intuitive platform for communicating with computers. This project, named Conjure, aims to take gesture-based interactions one step further by providing human-computer interactions with only a camera and hand recognition software. However, unlike the Xbox Kinect or Apple Vision Pro headset, which make use of depth sensors in addition to regular cameras, we attempt to use only a standard camera. Of course, the use case of Conjure is far simpler than that of VR headsets or game consoles, since the target platform this program is user-facing laptop webcams. We can further assume that most people interact with webcams in an egocentric view, that is, facing the camera with one prominent subject and a mostly static background.

The rest of this report will outline the features of Conjure along with instructions to get started, the model architectures explored and used within the program, some key results, and difficulties throughout the project.

\section{Usage}
To get started, clone the \href{https://github.com/Ant13731/conjure}{GitHub repository} and download all dependencies through \texttt{pip install -r requirements.txt}. Ensure the computer's webcam is plugged in and working, then run \texttt{python main.py}. Note that this project was tested with Python 3.12 on a Windows machine.

Upon startup, a GUI containing settings and other options should appear similar to Figure~\ref{fig:config}
\sidecaptionvpos{figure}{c}
\begin{SCfigure}[2][h]
    \centering
    \includegraphics[height=4cm]{config.png}
    \caption{Startup configuration UI. Click Start/Stop Camera to activate the virtual controller.}
    \label{fig:config}
\end{SCfigure}

The main interactions of Conjure are as follows, with coinciding images in Figure~\ref{fig:gestures}:
% TODO add in images of gestures, also add to README
\begin{itemize}
    \item Hand movement in the camera's 2D projection of 3D space coincides with mouse movement across a computer's monitor. For fine-grained movements and other interactions, Conjure creates a deadzone at the center of the screen, where all hand movements are ignored. Then, as a hand moves out of the deadzone toward the borders of the screen, the mouse will move with increasing velocity in the hand's general direction.
    \item Simulating a left click can be done by making an ``OK'' gesture towards the camera. This gesture is closest to the natural pinch gestures of VR headsets while being freely available in many datasets \cite{Alexander_2024, nuzhdin2024hagridv21mimagesstatic, zimmermann2017learningestimate3dhand, gesture_recognizer}.
    \item A right click conversely uses a ``peace'' sign, with the index and middle fingers extended and pointing away from each other.
    \item A click-and-hold motion makes use of a closing fist gesture. As long as the fist remains closed, the mouse will continue holding down the button. Moving around the camera space while maintaining a closed fist allows for drag-and-drop behaviours.
    \item Exiting the program is done with a ``reverse-stop'' motion, where the hand is fully extended, fingers close to one another, and the user's palm is facing away from the camera.
    \item Scrolling coincides with a two-finger-raised gesture, with up and down behaviour following the palm's direction - up when facing towards and down when facing away from the camera respectively. During testing, I found that scrolling behaviours worked best if it was only used in a specific area of the screen. Thus, this gesture only works within the scroll zone (by default, this is the right side of the mouse deadzone).
\end{itemize}

All gesture-specific actions may be changed with the GUI configuration, made in Tkinter. Other options, like movement sensitivity, deadzone sizes, etc., are also configurable through the UI.

\begin{figure}
    \centering
    \begin{subfigure}[T]{0.76\textwidth}
        \centering
        \includegraphics[width=.8\linewidth]{deadzone.png}
        \caption{The left deadzone prevents hand movement within that area from controlling the mouse. Any gestures will work in this area. The right scroll zone prevents mouse movement but enables scrolling movement. Pointing two fingers up when the index finger is within this zone will perform a scroll-up action.}
    \end{subfigure}
    \begin{subfigure}[T]{0.3\textwidth}
        \centering
        \includegraphics[width=.8\linewidth]{ok.png}
        \caption{OK symbol for left click.}
    \end{subfigure}\hfill
    \begin{subfigure}[T]{0.3\textwidth}
        \centering
        \includegraphics[width=.8\linewidth]{peace.png}
        \caption{Peace symbol for right click.}
    \end{subfigure}\hfill
    \begin{subfigure}[T]{0.3\textwidth}
        \centering
        \includegraphics[width=.8\linewidth]{fist.png}
        \caption{Clenching a hand will mirror click-and-hold behaviour.}
    \end{subfigure}
    \begin{subfigure}[T]{0.3\textwidth}
        \centering
        \includegraphics[width=.8\linewidth]{stop_inverted.png}
        \caption{A stop symbol with the palm facing away from the camera will exit the camera control program.}
    \end{subfigure}\hfill
    \begin{subfigure}[T]{0.3\textwidth}
        \centering
        \includegraphics[width=.8\linewidth]{two_up.png}
        \caption{Two fingers pointing up will scroll up (only in the right side scroll zone).}
    \end{subfigure}\hfill
    \begin{subfigure}[T]{0.3\textwidth}
        \centering
        \includegraphics[width=.8\linewidth]{two_up_inverted.png}
        \caption{Showing the back of the scroll-up gesture will scroll down (only in the right side scroll zone).}
    \end{subfigure}
    \caption{A collection of gestures described in section 2.}
    \label{fig:gestures}
\end{figure}

% likely will have the conjure folder zipped in avenue
% need to unzip, install requirements, run, also show how to train

\section{Model}


% model architecture: 3 of them, 1. SSD, 2. Regression CNN, 3. Classification CNN


\section{Results}
\subsection{Failed Experiments}
% First tried CNN, show loss curve
% TOMORROW: Use https://github.com/victordibia/handtracking as a pretrained model to just detect the bounding boxes of hands. then follow the google guide, crop photo (scale image up to original first model size, pad the difference to accommodate diff aspect ratios), predict coordinates, then pass coords to a fully connected NN for classification.
% Had a plan, not enough time to execute
% Final project, using pretrained 1 and 2 models, custom data passed to classifier model, show loss curve for classifier model
\subsection{Final Choice in Project}
\section{Conclusion}

\section{Usage Instructions}




\pagebreak
\nocite{*} % keeps all references, even those not referred to
\printbibliography %Prints bibliography
\end{document}

